{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35c8ae3",
   "metadata": {},
   "source": [
    "## CS336 - Lec2: PyTorch, resource accounting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c401",
   "metadata": {},
   "source": [
    "## Lec2: \n",
    "### 学习内容\n",
    "1. 训练模型需要的primitives\n",
    "2. tensors -> 模型 -> 优化器 -> 训练\n",
    "3. 效率/资源利用(Memory[GB] & Compute[FLOPs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e79951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac7de0",
   "metadata": {},
   "source": [
    "学习动机\n",
    "1. How long would it take to train a 70B parameter model on 15T tokens on 1024 H100s?   \n",
    "2. What's the largest model that can you can train on 8 H100s using AdamW (naively)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18b483",
   "metadata": {},
   "source": [
    "### Memory计算\n",
    "#### tensors基础\n",
    "Tensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建tensors的多种方式:\n",
    "x = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "x = torch.zeros(4, 8)  # 4x8的0矩阵\n",
    "x = torch.ones(4, 8) \n",
    "x = torch.randn(4, 8)  # 4x8大小的独立同分布正态(0, 1)样本点的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e716d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分配地址但没有值初始化:\n",
    "x = torch.empty(4, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc63b0d",
   "metadata": {},
   "source": [
    "#### tensors Memory\n",
    "几乎所有都被储存为floating point numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c0791",
   "metadata": {},
   "source": [
    "1. FLOAT32/fp32/单精度 (Default)\n",
    "    - float32由三部分组成：符号位（pos:31，最左侧）、指数位(pos:30-23)和尾数位(pos:22-0)\n",
    "    - 计算公式： $$(-1)^{\\text{符号位}}\\times 2^{\\text{指数位}-127}\\times (1+\\text{尾数位}/{2^{23}}) $$\n",
    "    - 比如 0.5，二进制是 $0.1_2$，规格化为$1.0\\times 2^{-1}$，所以符号=0，指数=126（二进制01111110），尾数=0（二进制000...）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2973d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine memory usage\n",
    "assert x.dtype == torch.float32 # Default type\n",
    "assert x.numel() == 4 * 8\n",
    "assert x.element_size() == 4  # Float is 4 bytes\n",
    "\n",
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "assert get_memory_usage(x) == 4 * 8 * 4  # 128 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One matrix in the FFN layer of GPT-3:\n",
    "assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024  # 2.3 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24402ba2",
   "metadata": {},
   "source": [
    "2. FLOAT16/fp16/半精度：符号15、指数14-10和尾数9-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caada6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内存减半，不适合很小/大的数\n",
    "x = torch.zeros(4, 8, dtype=torch.float16)\n",
    "assert x.element_size() == 2\n",
    "\n",
    "x = torch.tensor([1e-8], dtype=torch.float16)\n",
    "assert x == 0  # Underflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521cf75",
   "metadata": {},
   "source": [
    "3. bfloat16\n",
    "    - Google Brain于2018年提出此解决深度学习问题，符号15、指数14-7、尾数6-0\n",
    "    - 扩大范围，拥有和float32一样的动态范围，但是减小精度（深度学习中对噪声/精度不太敏感）\n",
    "    - 对于optimizer的参数等，仍需要高精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1e-8], dtype=torch.bfloat16)\n",
    "assert x != 0  # No underflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd96fea",
   "metadata": {},
   "source": [
    "4. FP8\n",
    "    - NVDA提出用于机器学习，H100支持"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a291f",
   "metadata": {},
   "source": [
    "5. Mixed Precision Training 混合精度训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0adda",
   "metadata": {},
   "source": [
    "### Compute计算\n",
    "#### gpu上的tensors\n",
    "默认tensor储存于cpu，因此需要手动调整到gpu上，并时常做sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查默认cpu\n",
    "assert x.device == torch.device('cpu')\n",
    "\n",
    "# 是否有cuda支持的gpu\n",
    "torch.cuda.is_available() \n",
    "\n",
    "# 有多少可用gpu设备\n",
    "num_gpus = torch.cuda.device_count()\n",
    "for i in range(num_gpus):\n",
    "    properties = torch.cuda.get_device_properties(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e659ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自动检测并部署cpu和gpu\n",
    "def get_device(index: int = 0) -> torch.device:\n",
    "    \"\"\"Try to use the GPU if possible, otherwise, use CPU.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(f\"cuda:{index}\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# 将x移到gpu\n",
    "y = x.to(get_device())\n",
    "z = torch.zeros(32, 32, device=get_device()) # 或直接声明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab493c72",
   "metadata": {},
   "source": [
    "#### tensor操作\n",
    "1. tensor storage\n",
    "2. tensor slicing（注意是对地址的引用，还是直接创建新的值）\n",
    "3. tensor element-wise operation\n",
    "4. tensor matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensor乘法\")\n",
    "\n",
    "x = torch.ones(16, 32)\n",
    "w = torch.ones(32, 2)\n",
    "y = x @ w # 矩阵乘法，*是element-wise乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1329b",
   "metadata": {},
   "source": [
    "但是在深度学习中，通常会对batch或sequence中的每个样本单独计算，所以为了方便，设置了broadcast机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b938e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4, 8, 16, 32)\n",
    "w = torch.ones(32, 2)\n",
    "y = x @ w\n",
    "\n",
    "assert y.size() == torch.Size([4, 8, 16, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435cbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, 3)  # batch, sequence, hidden\n",
    "y = torch.ones(2, 2, 3)  # batch, sequence, hidden\n",
    "z = x @ y.transpose(-2, -1)  # batch, sequence, sequence\n",
    "\n",
    "# 额外地，有更高效和清晰的做矩阵转置的操作。\n",
    "# Einops is a library for manipulating tensors where dimensions are named."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c5bf0",
   "metadata": {},
   "source": [
    "#### tensor操作的flops\n",
    "A floating-point operation (FLOP) is a basic operation like addition (x + y) or multiplication (x y).\n",
    "\n",
    "FLOPs: 计算完成需要的量\n",
    "FLOP/s: 表示硬件的计算速度的量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56bd025",
   "metadata": {},
   "source": [
    "直观感受\n",
    "- Training GPT-3 (2020) took 3.14e23 FLOPs.\n",
    "- Training GPT-4 (2023) is speculated to take 2e25 FLOPs\n",
    "\n",
    "- A100 has a peak performance of 312 teraFLOP/s\n",
    "- H100 has a peak performance of 1979 teraFLOP/s with sparsity, 50% without"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65860b9",
   "metadata": {},
   "source": [
    "对于线性模型Linear来说，假设有n个点，每个点有d维，线性模型最终将d维向量映射成k个outputs。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "\n",
    "device = get_device() # 或者cpu\n",
    "x = torch.ones(B, D, device=device)\n",
    "w = torch.randn(D, K, device=device)\n",
    "y = x @ w\n",
    "\n",
    "print(\"最终，大致需要的flops为\")\n",
    "actual_num_flops = 2 * B * D * K # 通常来说，在深度学习中，矩阵乘法比其他操作都要贵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefd188",
   "metadata": {},
   "source": [
    "- MFU 模型FLOPs利用率\n",
    "    - 简单定义: `mfu = actual_flop_per_sec / promised_flop_per_sec`\n",
    "        - `actual_flop_per_sec`通过`actual_num_flops`除以时间粗略得到\n",
    "    - 通常MFU大于0.5是好的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b9114",
   "metadata": {},
   "source": [
    "#### 梯度基础\n",
    "简单的例子，$$y = 0.5 (x * w - 5)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0a7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"前向传播路径\")\n",
    "\n",
    "x = torch.tensor([1., 2, 3])\n",
    "w = torch.tensor([1., 1, 1], requires_grad=True)  # Want gradient\n",
    "pred_y = x @ w\n",
    "loss = 0.5 * (pred_y - 5).pow(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"反向传播路径\")\n",
    "\n",
    "loss.backward()\n",
    "assert loss.grad is None\n",
    "assert pred_y.grad is None\n",
    "assert x.grad is None\n",
    "assert torch.equal(w.grad, torch.tensor([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0382b",
   "metadata": {},
   "source": [
    "#### 梯度FLOPs\n",
    "仍考虑线性模型: x --w1--> h1 --w2--> h2 -> loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e7daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "\n",
    "x = torch.ones(B, D, device=device)\n",
    "w1 = torch.randn(D, D, device=device, requires_grad=True)\n",
    "w2 = torch.randn(D, K, device=device, requires_grad=True)\n",
    "\n",
    "h1 = x @ w1\n",
    "h2 = h1 @ w2\n",
    "loss = h2.pow(2).mean()\n",
    "\n",
    "num_forward_flops = (2 * B * D * D) + (2 * B * D * K)  # 前向传播的flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe59736",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.retain_grad()  # 保留梯度，而不是计算完梯度就清内存\n",
    "h2.retain_grad() \n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4b8c3",
   "metadata": {},
   "source": [
    "链式求导\n",
    "1. h1.grad = d loss / d h1\n",
    "2. h2.grad = d loss / d h2\n",
    "3. w1.grad = d loss / d w1\n",
    "4. w2.grad = d loss / d w2\n",
    "5. x.grad = d loss/ dx, 输入梯度，x是输入不需要更新，但在深度网络或复杂的计算图中，为了继续向更早的层（如果有的话）传递梯度，必须计算对输入的导数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf925a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_backward_flops = 0\n",
    "\n",
    "print(\"对于w2，使用链式传播：w2.grad = h1.T @ h2.grad\")\n",
    "\n",
    "assert w2.grad.size() == torch.Size([D, K])\n",
    "assert h1.size() == torch.Size([B, D])\n",
    "assert h2.grad.size() == torch.Size([B, K])\n",
    "\n",
    "num_backward_flops += 2 * B * D * K\n",
    "\n",
    "num_backward_flops += 2 * B * K     # h2.grad, D一般远大于1，所以这部分计算量可忽略\n",
    "num_backward_flops += 2 * B * D * K # h1.grad = w2.T @ h2.grad\n",
    "num_backward_flops += 2 * B * D * D # w1.grad = x.T @ h1.grad\n",
    "num_backward_flops += 2 * B * D * D # x.grad = w1.T @ h1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab0b99",
   "metadata": {},
   "source": [
    "- 总的来说，对于深度学习这样矩阵乘法主导的模型计算，\n",
    "    - 前向传播: 2 * (# data points) * (# parameters) FLOPs\n",
    "    - 反向传播: 4 * (# data points) * (# parameters) FLOPs\n",
    "    - 加总为6倍\n",
    "    - 共享参数模型可能不成立"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a76b4",
   "metadata": {},
   "source": [
    "### Models\n",
    "#### Module参数和参数初始化 Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f63ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 16384\n",
    "output_dim = 32\n",
    "\n",
    "print(\"模型参数在PyTorch中被储存为nn.Parameter对象\")\n",
    "w = nn.Parameter(torch.randn(input_dim, output_dim))\n",
    "\n",
    "assert isinstance(w, torch.Tensor) \n",
    "assert type(w.data) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Parameter(torch.randn(input_dim))\n",
    "output = x @ w\n",
    "\n",
    "assert output.size() == torch.Size([output_dim])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b108701",
   "metadata": {},
   "source": [
    "- 上面`output`的每个元素的都会被放大`input_dim`的平方根大小。\n",
    "    - 因为我们每个元素都是由`x`和`torch.randn()`乘积的相加得到,而这两个变量中的每个元素都是独立$N(0,1)$,因此最终的`output`中每个元素为$N(0，\\text{input\\_dim})$\n",
    "    - 在大模型中，维数大可能会带来梯度爆炸，让训练过程不稳定\n",
    "    - 因此，通常需要初始化将其对`input_dim`不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3c3be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rescale by 1/sqrt(input_dim)\n",
    "w = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12e161b",
   "metadata": {},
   "source": [
    "其他初始化：Xavier初始化，或者直接将正态分布截断到[-3,3]区间等等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42b00f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.init.xavier_uniform_(w, gain=1.0)\n",
    "nn.init.xavier_normal_(w, gain=1.0)\n",
    "\n",
    "w = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, output_dim), \n",
    "    std=1 / np.sqrt(input_dim), a=-3, b=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3f3993",
   "metadata": {},
   "source": [
    "#### 搭建自定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    \"\"\"简单线性层\"\"\"\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__() # 手动调用nn.Module父类的init\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, output_dim) / np.sqrt(input_dim))\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x @ self.weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f36bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cruncher(nn.Module):\n",
    "    def __init__(self, dim: int, num_layers: int):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([Linear(dim, dim) for i in range(num_layers)])\n",
    "        self.final = Linear(dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 通过线性层\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # 通过最终层\n",
    "        x = self.final(x)\n",
    "        \n",
    "        # 去掉最后的维度\n",
    "        x = x.squeeze(-1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 64  # 隐藏层维度\n",
    "num_layers = 2\n",
    "model = Cruncher(dim=D, num_layers=num_layers)\n",
    "\n",
    "param_sizes = [\n",
    "    (name, param.numel()) # numel()返回tensor的元素总数\n",
    "    for name, param in model.state_dict().items()\n",
    "]\n",
    "num_parameters = sum(param.numel() for param in model.parameters())\n",
    "\n",
    "# 将模型部署到gpu\n",
    "device = get_device()\n",
    "model = model.to(device)\n",
    "\n",
    "# 检验结果\n",
    "B = 8  # Batch大小\n",
    "x = torch.randn(B, D, device=device)\n",
    "y = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fc8aba",
   "metadata": {},
   "source": [
    "为每个部分都设置随机种子，保证确定性复现结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be332426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb4744",
   "metadata": {},
   "source": [
    "#### 加载数据 Data Loading\n",
    "需要考虑非一次性加载完所有数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b68037",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.memmap(\"data.npy\", dtype=np.int32)\n",
    "\n",
    "def get_batch(data: np.array, batch_size: int, sequence_length: int, device: str) -> torch.Tensor:\n",
    "    # Sample batch_size random positions into data.\n",
    "    start_indices = torch.randint(len(data) - sequence_length, (batch_size,))\n",
    "\n",
    "    # Index into the data.\n",
    "    x = torch.tensor([data[start:start + sequence_length] for start in start_indices])\n",
    "\n",
    "    # 待完成......\n",
    "\n",
    "B = 2  # Batch大小\n",
    "L = 4  # 序列长度\n",
    "x = get_batch(data, batch_size=B, sequence_length=L, device=get_device())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35736696",
   "metadata": {},
   "source": [
    "#### 优化器 Optimizer\n",
    "- momentum = SGD + exponential averaging of grad\n",
    "- AdaGrad = SGD + averaging by grad^2\n",
    "- RMSProp = AdaGrad + exponentially averaging of grad^2\n",
    "- Adam = RMSProp + momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb214f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现AdaGrad\n",
    "from typing import Iterable # 可在def定义出直接指明是迭代器，但不限定迭代器种类\n",
    "\n",
    "class SGD(torch.optim.Optimizer):\n",
    "    def __init__(self, params: Iterable[nn.Parameter], lr: float = 0.01):\n",
    "        super(SGD, self).__init__(params, dict(lr=lr))\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                grad = p.grad.data\n",
    "                p.data -= lr * grad\n",
    "\n",
    "class AdaGrad(torch.optim.Optimizer):\n",
    "    def __init__(self, params: Iterable[nn.Parameter], lr: float = 0.01):\n",
    "        super(AdaGrad, self).__init__(params, dict(lr=lr))\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                # 优化器状态\n",
    "                state = self.state[p]\n",
    "                grad = p.grad.data\n",
    "                # 获得梯度的平方g2\n",
    "                g2 = state.get(\"g2\", torch.zeros_like(grad))\n",
    "                # 更新优化器状态\n",
    "                g2 += torch.square(grad)\n",
    "                state[\"g2\"] = g2\n",
    "                # 更新参数\n",
    "                p.data -= lr * grad / torch.sqrt(g2 + 1e-5)\n",
    "\n",
    "\n",
    "optimizer = AdaGrad(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器实际使用在loss.backward()后\n",
    "x = torch.randn(B, D, device=get_device())\n",
    "y = torch.tensor([4., 5.], device=device)\n",
    "pred_y = model(x)\n",
    "loss = F.mse_loss(input=pred_y, target=y)\n",
    "loss.backward()\n",
    "\n",
    "optimizer.step()\n",
    "state = model.state_dict()  \n",
    "\n",
    "# 清空内存\n",
    "optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e44abd",
   "metadata": {},
   "source": [
    "1. 计算模型的内存消耗：\n",
    "- 参数：num_parameters = (D * D * num_layers) + D\n",
    "- 激活值（h1,h2）：num_activations = B * D * num_layers\n",
    "- 梯度：num_gradients = num_parameters\n",
    "- AdaGrad优化器状态：num_optimizer_states = num_parameters\n",
    "\n",
    "\\\n",
    "总共内存 = 4[用fp32] * (num_parameters + num_activations + num_gradients + num_optimizer_states)\n",
    "\n",
    "2. 计算消耗： flops = 6 * B * num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9585572",
   "metadata": {},
   "source": [
    "Transformer的内存和计算消耗：\n",
    "1. [Transformer内存消耗](https://erees.dev/transformer-memory/)\n",
    "2. [Transformer计算消耗](https://www.adamcasson.com/posts/transformer-flops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5640759f",
   "metadata": {},
   "source": [
    "#### 训练 Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf99489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop():\n",
    "    # 从linear function with weights (0, 1, 2, ..., D-1)生成数据\n",
    "    D = 16\n",
    "    true_w = torch.arange(D, dtype=torch.float32, device=get_device())\n",
    "    def get_batch(B: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = torch.randn(B, D).to(device)\n",
    "        true_y = x @ true_w\n",
    "        return (x, true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4894b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(name: str, get_batch,\n",
    "          D: int, num_layers: int,\n",
    "          B: int, num_train_steps: int, lr: float):\n",
    "    model = Cruncher(dim=D, num_layers=num_layers).to(get_device())\n",
    "    optimizer = SGD(model.parameters(), lr=0.01)\n",
    "    for _ in range(num_train_steps):\n",
    "        # 获取数据\n",
    "        x, y = get_batch(B=B)\n",
    "        # 前向(计算loss)\n",
    "        pred_y = model(x)\n",
    "        loss = F.mse_loss(pred_y, y)\n",
    "        # 反向(计算梯度)\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "# 做一次简单运行 \n",
    "train(\"simple\", get_batch, D=D, num_layers=0, B=4, num_train_steps=10, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc5e909",
   "metadata": {},
   "source": [
    "#### 检查点 Checkpoint\n",
    "训练时间长，需要间断地保存模型和优化器参数到disk中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Cruncher(dim=64, num_layers=3).to(get_device())\n",
    "optimizer = AdaGrad(model.parameters(), lr=0.01)\n",
    "\n",
    "# 保存checkpoint\n",
    "checkpoint = {\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "}\n",
    "torch.save(checkpoint, \"model_checkpoint.pt\")\n",
    "\n",
    "# 加载checkpoint\n",
    "loaded_checkpoint = torch.load(\"model_checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d36b51",
   "metadata": {},
   "source": [
    "#### 混合精度训练\n",
    "- 成熟的方案：前向用bf16、fp8，反向用fp32\n",
    "- PyTorch有AMP库用于自动混合精度\n",
    "- NVIDIA支持fp8用于线性层"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
