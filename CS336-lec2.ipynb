{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b35c8ae3",
   "metadata": {},
   "source": [
    "## CS336 - Lec1(tech)+Lec2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccfff7",
   "metadata": {},
   "source": [
    "## Tokenizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b837c401",
   "metadata": {},
   "source": [
    "## Overview of Lec2:\n",
    "- primitives needed to train a model\n",
    "- tensors to models to optimizers to training loop\n",
    "- efficiency/use of resources(Memory GB & Compute FLOPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e79951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac7de0",
   "metadata": {},
   "source": [
    "def motivating_questions():\n",
    "\n",
    "1. How long would it take to train a 70B parameter model on 15T tokens on 1024 H100s?   \n",
    "2. What's the largest model that can you can train on 8 H100s using AdamW (naively)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18b483",
   "metadata": {},
   "source": [
    "### Memory Accounting\n",
    "\n",
    "#### tensors_basics()\n",
    "Tensors are the basic building block for storing everything: parameters, gradients, optimizer state, data, activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb35a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tensors in multiple ways:\n",
    "x = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "x = torch.zeros(4, 8)  # 4x8 matrix of all zeros \n",
    "x = torch.ones(4, 8)  # 4x8 matrix of all ones \n",
    "x = torch.randn(4, 8)  # 4x8 matrix of iid Normal(0, 1) samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e716d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate but don't initialize the values:\n",
    "x = torch.empty(4, 8)  # 4x8 matrix of uninitialized values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc63b0d",
   "metadata": {},
   "source": [
    "#### def tensors_memory()\n",
    "almost everything are stored as floating point numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934c0791",
   "metadata": {},
   "source": [
    "FLOAT32/fp32/single precision (Default)\n",
    "- float32由三部分组成：符号位（pos:31，最左侧）、指数位(pos:30-23)和尾数位(pos:22-0)\n",
    "- 计算公式： $$(-1)^{\\text{符号位}}\\times 2^{\\text{指数位}-127}\\times (1+\\text{尾数位}/{2^{23}}) $$\n",
    "- 比如 0.5，二进制是 $0.1_2$，规格化为$1.0\\times 2^{-1}$，所以符号=0，指数=126（二进制01111110），尾数=0（二进制000...）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2973d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine memory usage\n",
    "assert x.dtype == torch.float32 # Default type\n",
    "assert x.numel() == 4 * 8\n",
    "assert x.element_size() == 4  # Float is 4 bytes\n",
    "\n",
    "def get_memory_usage(x: torch.Tensor):\n",
    "    return x.numel() * x.element_size()\n",
    "\n",
    "assert get_memory_usage(x) == 4 * 8 * 4  # 128 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543fb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One matrix in the FFN layer of GPT-3:\n",
    "assert get_memory_usage(torch.empty(12288 * 4, 12288)) == 2304 * 1024 * 1024  # 2.3 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24402ba2",
   "metadata": {},
   "source": [
    "FLOAT16/fp16/half precision：符号15、指数14-10和尾数9-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caada6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 内存减半，不适合很小/大的数\n",
    "x = torch.zeros(4, 8, dtype=torch.float16)\n",
    "assert x.element_size() == 2\n",
    "\n",
    "x = torch.tensor([1e-8], dtype=torch.float16)\n",
    "assert x == 0  # Underflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4521cf75",
   "metadata": {},
   "source": [
    "bfloat16\n",
    "- Google Brain于2018年提出此解决深度学习问题，符号15、指数14-7、尾数6-0\n",
    "- 扩大范围，拥有和float32一样的动态范围，但是减小精度（深度学习中对噪声/精度不太敏感）\n",
    "- 对于optimizer的参数等，仍需要高精度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af7330",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1e-8], dtype=torch.bfloat16)\n",
    "assert x != 0  # No underflow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd96fea",
   "metadata": {},
   "source": [
    "FP8\n",
    "- NVDA提出用于机器学习，H100支持"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a291f",
   "metadata": {},
   "source": [
    "Mixed Precision Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c0adda",
   "metadata": {},
   "source": [
    "### Compute Accounting\n",
    "\n",
    "#### tensors_on_gpu() \n",
    "默认tensor储存于cpu，因此需要手动调整到gpu上，并时常做sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389cf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查默认cpu\n",
    "assert x.device == torch.device('cpu')\n",
    "\n",
    "# 是否有cuda支持的gpu\n",
    "torch.cuda.is_available() \n",
    "\n",
    "# 有多少可用gpu设备\n",
    "num_gpus = torch.cuda.device_count()\n",
    "for i in range(num_gpus):\n",
    "    properties = torch.cuda.get_device_properties(i)\n",
    "\n",
    "# 将x移到gpu\n",
    "y = x.to(\"cuda:0\")\n",
    "z = torch.zeros(32, 32, device=\"cuda:0\") # 或直接声明"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab493c72",
   "metadata": {},
   "source": [
    "#### tensor operations\n",
    "1. tensor storage\n",
    "2. tensor slicing（注意是对地址的引用，还是直接创建新的值）\n",
    "3. tensor element-wise operation\n",
    "4. tensor matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe40354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor乘法\n",
    "x = torch.ones(16, 32)\n",
    "w = torch.ones(32, 2)\n",
    "y = x @ w # 矩阵乘法，*是element-wise乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1329b",
   "metadata": {},
   "source": [
    "但是在深度学习中，通常会对batch或sequence中的每个样本单独计算，所以为了方便，设置了broadcast机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b938e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(4, 8, 16, 32)\n",
    "w = torch.ones(32, 2)\n",
    "y = x @ w\n",
    "\n",
    "assert y.size() == torch.Size([4, 8, 16, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435cbf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(2, 2, 3)  # batch, sequence, hidden\n",
    "y = torch.ones(2, 2, 3)  # batch, sequence, hidden\n",
    "z = x @ y.transpose(-2, -1)  # batch, sequence, sequence\n",
    "\n",
    "# 额外地，有更高效和清晰的做矩阵转置的操作\n",
    "# Einops is a library for manipulating tensors where dimensions are named."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c5bf0",
   "metadata": {},
   "source": [
    "#### tensor operation flops\n",
    "A floating-point operation (FLOP) is a basic operation like addition (x + y) or multiplication (x y).\n",
    "\n",
    "FLOPs: 计算完成需要的量\n",
    "FLOP/s: 表示硬件的计算速度的量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56bd025",
   "metadata": {},
   "source": [
    "直观感受\n",
    "- Training GPT-3 (2020) took 3.14e23 FLOPs.\n",
    "- Training GPT-4 (2023) is speculated to take 2e25 FLOPs\n",
    "\n",
    "- A100 has a peak performance of 312 teraFLOP/s\n",
    "- H100 has a peak performance of 1979 teraFLOP/s with sparsity, 50% without"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65860b9",
   "metadata": {},
   "source": [
    "对于线性模型Linear来说，假设有n个点，每个点有d维，线性模型最终将d维向量映射成k个outputs。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407bde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    B = 16384  # Number of points\n",
    "    D = 32768  # Dimension\n",
    "    K = 8192   # Number of outputs\n",
    "else:\n",
    "    B = 1024\n",
    "    D = 256\n",
    "    K = 64\n",
    "\n",
    "device = \"cuda:0\" # 或者cpu\n",
    "x = torch.ones(B, D, device=device)\n",
    "w = torch.randn(D, K, device=device)\n",
    "y = x @ w\n",
    "\n",
    "# 最终，大致需要的flops为\n",
    "actual_num_flops = 2 * B * D * K\n",
    "# 通常来说，在深度学习中，矩阵乘法比其他操作都要贵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefd188",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
