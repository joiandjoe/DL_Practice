{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2cc9b59",
   "metadata": {},
   "source": [
    "## CS336 - Lec1: Overview, tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc4ec2c",
   "metadata": {},
   "source": [
    "## Lec1: \n",
    "### è¯¾ç¨‹ä»‹ç»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baa9106",
   "metadata": {},
   "source": [
    "#### è¯¾ç¨‹æ”¶è·\n",
    "There are three types of knowledge:\n",
    "    \n",
    "1. *Mechanics: how things work (what a Transformer is, how model parallelism leverages GPUs)\n",
    "2. *Mindset: squeezing the most out of the hardware, taking scale seriously (scaling laws)\n",
    "3. Intuitions: which data and modeling decisions yield good accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c097f653",
   "metadata": {},
   "source": [
    "#### ç›®æ ‡\n",
    "The bitter lesson\n",
    "- accuracy = efficiency x resources\n",
    "- Framing: what is the best model one can build given a certain compute and data budget?\n",
    "In other words, maximize efficiency!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc61e53",
   "metadata": {},
   "source": [
    "#### Current Landscape\n",
    "1. Pre-neural(before 2010s)\n",
    "    - Language model to measure the entropy of English [Shannon 1950]\n",
    "    - Lots of work on n-gram language models (for machine translation, speech recognition) [Brants+ 2007]\n",
    "2. Neural ingredients (2010s)    \n",
    "    - First neural language model [Bengio+ 2003]\n",
    "    - Sequence-to-sequence modeling (for machine translation) [Sutskever+ 2014]\n",
    "    - Adam optimizer [Kingma+ 2014]\n",
    "    - Attention mechanism (for machine translation) [Bahdanau+ 2014]\n",
    "    - Transformer architecture (for machine translation) [Vaswani+ 2017]\n",
    "    - Mixture of experts [Shazeer+ 2017]\n",
    "    - Model parallelism [Huang+ 2018][Rajbhandari+ 2019][Shoeybi+ 2019]\n",
    "3. Early foundation models (late 2010s)\n",
    "    - ELMo: pretraining with LSTMs, fine-tuning helps tasks [Peters+ 2018]\n",
    "    - BERT: pretraining with Transformer, fine-tuning helps tasks [Devlin+ 2018]\n",
    "    - Google's T5 (11B): cast everything as text-to-text [Raffel+ 2019]\n",
    "4. Embracing scaling, more closed\n",
    "    - OpenAI's GPT-2 (1.5B): fluent text, first signs of zero-shot, staged release [Radford+ 2019]\n",
    "    - Scaling laws: provide hope / predictability for scaling [Kaplan+ 2020]\n",
    "    - OpenAI's GPT-3 (175B): in-context learning, closed [Brown+ 2020]\n",
    "    - Google's PaLM (540B): massive scale, undertrained [Chowdhery+ 2022]\n",
    "    - DeepMind's Chinchilla (70B): compute-optimal scaling laws [Hoffmann+ 2022]\n",
    "5. Open models    \n",
    "    - EleutherAI's open datasets (The Pile) and models (GPT-J) [Gao+ 2020][Wang+ 2021]\n",
    "    - Meta's OPT (175B): GPT-3 replication, lots of hardware issues [Zhang+ 2022]\n",
    "    - Hugging Face / BigScience's BLOOM: focused on data sourcing [Workshop+ 2022]\n",
    "    - Meta's Llama models [Touvron+ 2023][Touvron+ 2023][Grattafiori+ 2024]\n",
    "    - Alibaba's Qwen models [Qwen+ 2024]\n",
    "    - DeepSeek's models [DeepSeek-AI+ 2024][DeepSeek-AI+ 2024][DeepSeek-AI+ 2024]\n",
    "    - AI2's OLMo 2 [Groeneveld+ 2024][OLMo+ 2024]\n",
    "6. Today's frontier models\n",
    "    - [OpenAI's o3](https://openai.com/index/openai-o3-mini/)\n",
    "    - [Anthropic's Claude Sonnet 3.7](https://www.anthropic.com/news/claude-3-7-sonnet)\n",
    "    - [xAI's Grok 3](https://x.ai/news/grok-3)\n",
    "    - [Google's Gemini 2.5](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n",
    "    - [Meta's Llama 3.3](https://ai.meta.com/blog/meta-llama-3/)\n",
    "    - [DeepSeek's r1]\n",
    "    - [Alibaba's Qwen 2.5 Max](https://qwenlm.github.io/blog/qwen2.5-max/)\n",
    "    - [Tencent's Hunyuan-T1](https://tencent.github.io/llm.hunyuan.T1/README_EN.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a8a39",
   "metadata": {},
   "source": [
    "#### é€‚ç”¨äººç¾¤\n",
    "- Why you should take this course \n",
    "    - You have an obsessive need to understand how things work.\n",
    "    - You want to build up your research engineering muscles.\n",
    "- Why you should not take this course\n",
    "    - You actually want to get research done this quarter.(Talk to your advisor.)\n",
    "    - You are interested in learning about the hottest new techniques in AI (e.g., multimodality, RAG, etc.).(You should take a seminar class for that.)\n",
    "    - You want to get good results on your own application domain.(You should just prompt or fine-tune an existing model.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb5f4ac",
   "metadata": {},
   "source": [
    "#### è¯¾ç¨‹ç»„æˆ\n",
    "- It's all about efficiency. Efficiency drives design decisions.\n",
    "    1. Basics: Tokenization, Architecture, Loss Function, Optimizer, Learning rate\n",
    "    2. Systems: Kernels, Parallelism, Quantilization, Activation checkpoint, CPU offloading, Inference\n",
    "    3. Scaling Laws: Scaling sequence, Model complexity, Loss metric, Parametric form\n",
    "    4. Data: Evaluation, Curation, Transformation, Filtering, Deduplication, Mixing\n",
    "    5. Alignment: Supervised fine-tuning, Reinforment Learning, Preference data, Synthetic data, Verifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952e2ceb",
   "metadata": {},
   "source": [
    "##### åŸºç¡€ Basics\n",
    "- ç›®æ ‡: å¾—åˆ°èƒ½å¤Ÿè¿è¡Œçš„åŸºç¡€ç‰ˆæœ¬æµç¨‹ \n",
    "- ç»„æˆéƒ¨åˆ†: åˆ†è¯, æ¨¡å‹æ¶æ„, è®­ç»ƒ\n",
    "1. åˆ†è¯ Tokenization\n",
    "    - Tokenizers convert between strings and sequences of integers (tokens)\n",
    "    - Intuition: break up string into popular segments\n",
    "    - This course: Byte-Pair Encoding (BPE) tokenizer\n",
    "2. æ¨¡å‹æ¶æ„\n",
    "    - Starting point: original Transformer \n",
    "    - å˜ä½“ï¼š\n",
    "        - Activation functions: ReLU, SwiGLU\n",
    "        - Positional encodings: sinusoidal, RoPE\n",
    "        - Normalization: LayerNorm, RMSNorm\n",
    "        - Placement of normalization: pre-norm versus post-norm\n",
    "        - MLP: dense, mixture of experts\n",
    "        - Attention: full, sliding window, linear\n",
    "        - Lower-dimensional attention: group-query attention (GQA), multi-head latent attention (MLA)\n",
    "        - State-space models: Hyena\n",
    "3. è®­ç»ƒ\n",
    "    - Optimizer (e.g., AdamW, Muon, SOAP)\n",
    "    - Learning rate schedule (e.g., cosine, WSD)\n",
    "    - Batch size (e..g, critical batch size)\n",
    "    - Regularization (e.g., dropout, weight decay)\n",
    "    - Hyperparameters (number of heads, hidden dimension): grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31795c",
   "metadata": {},
   "source": [
    "##### ç³»ç»Ÿ Systems\n",
    "- ç›®æ ‡: å‹æ¦¨ç¡¬ä»¶æ€§èƒ½\n",
    "- ç»„æˆéƒ¨åˆ†: è®¡ç®—æ ¸ kernels, å¹¶è¡Œ parallelism, æ¨ç† inference\n",
    "\n",
    "...\n",
    "1. Kernels \n",
    "    - Trick: organize computation to maximize utilization of GPUs by minimizing data movement\n",
    "    - Write kernels in CUDA/Triton/CUTLASS/ThunderKittens\n",
    "2. Parallelism\n",
    "    - What if we have multiple GPUs (8 A100s)?\n",
    "    - Data movement between GPUs is even slower, but same 'minimize data movement' principle holds\n",
    "    - Use collective operations (e.g., gather, reduce, all-reduce)\n",
    "    - Shard (parameters, activations, gradients, optimizer states) across GPUs\n",
    "    - How to split computation: {data,tensor,pipeline,sequence} parallelism\n",
    "3. Inference\n",
    "    - Goal: generate tokens given a prompt (needed to actually use models!)\n",
    "    - Inference is also needed for reinforcement learning, test-time compute, evaluation\n",
    "    - Globally, inference compute (every use) exceeds training compute (one-time cost)\n",
    "    - Two phases: prefill and decode\n",
    "        - Prefill (similar to training): tokens are given, can process all at once (compute-bound)\n",
    "        - Decode: need to generate one token at a time (memory-bound)\n",
    "        - Methods to speed up decoding:\n",
    "            - Use cheaper model (via model pruning, quantization, distillation)\n",
    "            - Speculative decoding: use a cheaper \"draft\" model to generate multiple tokens, then use the full model to score in parallel (exact decoding!)\n",
    "            - Systems optimizations: KV caching, batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53c30f",
   "metadata": {},
   "source": [
    "##### Scaling Laws\n",
    "- ç›®æ ‡: do experiments at small scale, predict hyperparameters/loss at large scale\n",
    "- é—®é¢˜: given a FLOPs budget (C), use a bigger model (N) or train on more tokens (D)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54efb939",
   "metadata": {},
   "source": [
    "##### æ•°æ® Data\n",
    "- Question: What capabilities do we want the model to have? Multilingual? Code? Math?\n",
    "\n",
    "1. Evaluation\n",
    "    - Perplexity: textbook evaluation for language models\n",
    "    - Standardized testing (e.g., MMLU, HellaSwag, GSM8K)\n",
    "    - Instruction following (e.g., AlpacaEval, IFEval, WildBench)\n",
    "    - Scaling test-time compute: chain-of-thought, ensembling\n",
    "    - LM-as-a-judge: evaluate generative tasks\n",
    "    - Full system: RAG, agents\n",
    "2. Data curation\n",
    "    - Data does not just fall from the sky.\n",
    "    - Sources: webpages crawled from the Internet, books, arXiv papers, GitHub code, etc.\n",
    "    - Appeal to fair use to train on copyright data?\n",
    "    - Might have to license data (e.g., Google with Reddit data)\n",
    "    - Formats: HTML, PDF, directories (not text!)\n",
    "3. Data processing\n",
    "    - Transformation: convert HTML/PDF to text (preserve content, some structure, rewriting)\n",
    "    - Filtering: keep high quality data, remove harmful content (via classifiers)\n",
    "    - Deduplication: save compute, avoid memorization; use Bloom filters or MinHash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b85449",
   "metadata": {},
   "source": [
    "##### å¯¹é½ Alignment\n",
    "- So far, a base model is raw potential, very good at completing the next token.\n",
    "- Alignment makes the model actually useful.\n",
    "- Goals of alignment:\n",
    "    - Get the language model to follow instructions\n",
    "    - Tune the style (format, length, tone, etc.)\n",
    "    - Incorporate safety (e.g., refusals to answer harmful questions)\n",
    "- Two phases:\n",
    "    - `supervised_finetuning()`\n",
    "    - `learning_from_feedback()`\n",
    "\n",
    "1. Supervised finetuning (SFT)\n",
    "    - Instruction data: (prompt, response) pairs\n",
    "    - ç¤ºä¾‹ï¼š\n",
    "        ```python\n",
    "        sft_data: list[ChatExample] = [\n",
    "            ChatExample(\n",
    "                turns=[\n",
    "                    Turn(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "                    Turn(role=\"user\", content=\"What is 1 + 1?\"),\n",
    "                    Turn(role=\"assistant\", content=\"The answer is 2.\"),\n",
    "                ],\n",
    "            ),\n",
    "        ]\n",
    "        ```\n",
    "    - Data often involves human annotation.\n",
    "    - Intuition: base model already has the skills, just need few examples to surface them.\n",
    "    - Supervised learning: fine-tune model to maximize p(response | prompt).\n",
    "2. Now we have a preliminary instruction following model.Let's make it better without expensive annotation.\n",
    "3. åå¥½æ•°æ® Preference data\n",
    "    -  Data: generate multiple responses using model (e.g., [A, B]) to a given prompt.\n",
    "    - User provides preferences (e.g., A < B or A > B).\n",
    "    - ç¤ºä¾‹ï¼š\n",
    "        ```python\n",
    "        preference_data: list[PreferenceExample] = [\n",
    "            PreferenceExample(\n",
    "                history=[\n",
    "                    Turn(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "                    Turn(role=\"user\", content=\"What is the best way to train a language model?\"),\n",
    "                ],\n",
    "                response_a=\"You should use a large dataset and train for a long time.\",\n",
    "                response_b=\"You should use a small dataset and train for a short time.\",\n",
    "                chosen=\"a\",\n",
    "            )\n",
    "        ]\n",
    "        ```\n",
    "4. Verifiers\n",
    "    - Formal verifiers (e.g., for code, math)\n",
    "    - Learned verifiers: train against an LM-as-a-judge\n",
    "5. Algorithms\n",
    "    - Proximal Policy Optimization (PPO) from reinforcement learning\n",
    "    - Direct Policy Optimization (DPO): for preference data, simpler\n",
    "    - Group Relative Preference Optimization (GRPO): remove value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484dfadb",
   "metadata": {},
   "source": [
    "### åˆ†è¯å™¨ Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6689517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L23\n",
    "GPT2_TOKENIZER_REGEX = \\\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b97440",
   "metadata": {},
   "source": [
    "#### åˆ†è¯æ€»ç»“\n",
    "å‚è€ƒ Andrej Karpathy[è§†é¢‘](https://www.youtube.com/watch?v=zduSFxRajkE)\n",
    "- åˆ†è¯å™¨å°†`strings`è½¬æ¢ä¸º`tokens`/`indices`/`integer`\n",
    "- æ–¹å¼ï¼šåŸºäºCharacterã€åŸºäºByteã€åŸºäºWordçš„åˆ†è¯\n",
    "- è‡³ä»Šï¼ŒByte Pair Encoding BPEåˆ†è¯å™¨æ•ˆæœæœ€ä½³"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d7d935",
   "metadata": {},
   "source": [
    "Introï¼š\n",
    "- ç¤ºä¾‹ï¼šåŸæ–‡æœ¬ string = \"Hello, ğŸŒ! ä½ å¥½!\" <=> åˆ†è¯åæ–‡æœ¬è¡¨ç¤ºä¸º indices = [15496, 11, 995, 0]\n",
    "- åˆ†è¯å™¨å¯ä»¥å®ç°encodeå’Œdecodeçš„åŠŸèƒ½çš„ç±»\n",
    "- å…¶ä¸­çš„è¯è¡¨å¤§å°æ˜¯å¯èƒ½çš„tokensçš„æ•°é‡\n",
    "- å°è¯•ç”¨[GPT2çš„åˆ†è¯å™¨](https://tiktokenizer.vercel.app/?encoder=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4019f9",
   "metadata": {},
   "source": [
    "1. åŸºäºCharacterçš„åˆ†è¯\n",
    "- Unicodeçš„string\n",
    "```python\n",
    "    assert ord(\"a\") == 97\n",
    "    assert ord(\"ğŸŒ\") == 127757\n",
    "\n",
    "    assert chr(97) == \"a\"\n",
    "    assert chr(127757) == \"ğŸŒ\"\n",
    "```\n",
    "- é—®é¢˜ï¼šè¯è¡¨å¾ˆå¤§ï¼Œä¸”å¤§å¤šæ•°è¯ä½¿ç”¨å¾ˆå°‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92de3d3c",
   "metadata": {},
   "source": [
    "2. åŸºäºByteçš„åˆ†è¯\n",
    "- Unicodeçš„stringå¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸²å­—èŠ‚ï¼ˆ0-255ï¼‰ï¼Œæœ€å¸¸è§çš„Unicodeç¼–ç æ˜¯UTF-8\n",
    "- é—®é¢˜ï¼šåŸå¥å­çš„åºåˆ—é•¿åº¦ä¼šéå¸¸é•¿"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea955490",
   "metadata": {},
   "source": [
    "3. åŸºäºWordçš„åˆ†è¯\n",
    "- å¯ä»¥ç›´æ¥ç”¨regexæ¥åˆ†è¯ï¼Œä½†æ˜¯é—®é¢˜å’ŒCharçš„åˆ†è¯ä¸€æ ·ä¸”éœ€è¦æ²¡è§è¿‡çš„è¯çš„token UNK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77138e",
   "metadata": {},
   "source": [
    "4. BPEåˆ†è¯\n",
    "- æƒ³æ³•: ç”¨åŸå§‹æ–‡æœ¬è®­ç»ƒåˆ†è¯å™¨ï¼Œè‡ªåŠ¨ç¡®å®šè¯æ±‡è¡¨ã€‚å¸¸è§çš„å­—ç¬¦åºåˆ—è¡¨ç¤ºä¸ºå•ä¸ªtokenï¼Œç½•è§çš„å­—ç¬¦åºåˆ—è¡¨ç¤ºä¸ºå¤šä¸ªtoken\n",
    "- è¿‡ç¨‹ï¼š\n",
    "    1. å¼€å§‹å°†æ¯ä¸ªbyteçœ‹ä½œä¸€ä¸ªtoken/word-basedåˆ†è¯\n",
    "    2. è¿­ä»£ï¼Œæ¯æ¬¡åˆå¹¶æœ€å¸¸å‡ºç°çš„ç›¸é‚»çš„token pairï¼Œä½œä¸ºæ–°çš„tokenå€¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d1b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "string = \"the cat in the hat\"\n",
    "\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]     # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "\n",
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:  \n",
    "    indices = list(map(int, string.encode(\"utf-8\")))  \n",
    "    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n",
    "    for i in range(num_merges):\n",
    "        #Count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n",
    "            counts[(index1, index2)] += 1  # @inspect counts\n",
    "        #Find the most common pair.\n",
    "        pair = max(counts, key=counts.get) \n",
    "        index1, index2 = pair\n",
    "        #Merge that pair.\n",
    "        new_index = 256 + i \n",
    "        merges[pair] = new_index\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]  \n",
    "        indices = merge(indices, pair, new_index)  \n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
